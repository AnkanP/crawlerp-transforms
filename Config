sc = spark.sparkContext
hadoop_conf = sc._jsc.hadoopConfiguration()

hadoop_conf.set("fs.s3.maxRetries", "20")
hadoop_conf.set("fs.s3a.retry.limit", "20")
hadoop_conf.set("fs.s3a.attempts.maximum", "20")
hadoop_conf.set("fs.s3a.connection.maximum", "100")
hadoop_conf.set("fs.s3a.connection.timeout", "5000")
hadoop_conf.set("fs.s3a.retry.interval", "5000")
