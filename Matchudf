import re
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Collect config into Python list of tuples
config_rules = [
    (row['search_key'], row['business_key'], row['newjsonpath'], row['description'])
    for row in config_df.collect()
]

# Define the matcher
def match_and_enrich(jsonkey, jsonpath):
    for key_pattern, biz, newpath, desc in config_rules:
        if re.match(key_pattern, jsonkey) and re.match(newpath, jsonpath):
            return (1, biz, newpath, desc)  # match found
    return (0, None, None, None)  # no match

# Define return schema
schema = StructType([
    StructField("matched_flag", IntegerType(), True),
    StructField("business_key", StringType(), True),
    StructField("newjsonpath", StringType(), True),
    StructField("description", StringType(), True),
])

# Register as UDF
match_udf = udf(match_and_enrich, schema)

# Apply to dataframe
final_df = (
    df.withColumn("result", match_udf(col("jsonkey"), col("jsonpath")))
      .select("jsonkey", "jsonpath", "result.*")
)

final_df.show(truncate=False)
