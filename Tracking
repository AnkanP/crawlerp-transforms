import threading
import time
from datetime import datetime

def start_spark_monitor(spark, interval: int = 30):
    """
    Periodically prints Spark executor/task statistics to the Glue job logs.
    Compatible with Glue 5.0 (Spark 3.5).

    Parameters
    ----------
    spark : SparkSession
        Active Spark session.
    interval : int
        Time interval (seconds) between log updates.
    """
    tracker = spark.sparkContext.statusTracker()

    def monitor():
        while True:
            try:
                # --- Executors (via JVM handle) ---
                mem_status = spark._jsc.sc().getExecutorMemoryStatus()
                num_exec = len(mem_status) - 1  # subtract driver

                # --- Active stages / tasks ---
                active_tasks = 0
                for sid in tracker.getActiveStageIds():
                    s = tracker.getStageInfo(sid)
                    if s:
                        active_tasks += s.numActiveTasks()

                avg_tasks = active_tasks / num_exec if num_exec > 0 else 0
                arrow_batch = spark.conf.get(
                    "spark.sql.execution.arrow.maxRecordsPerBatch", "default"
                )

                now = datetime.now().strftime("%H:%M:%S")
                print(
                    f"[{now}] [SparkMonitor] "
                    f"Executors={num_exec:3d} | "
                    f"ActiveTasks={active_tasks:4d} | "
                    f"AvgTasks/Exec={avg_tasks:4.1f} | "
                    f"ArrowBatch={arrow_batch}"
                )

                time.sleep(interval)

            except Exception as e:
                print(f"[SparkMonitor] stopped: {e}")
                break

    threading.Thread(target=monitor, daemon=True).start()
